services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"  # <--- Expose this so you can check ZK connectivity
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 10s
      retries: 5
      start_period: 10s

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    ports:
      - "9092:9092"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8089:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      # Optional: enable message viewing (disabled by default for safety)
      - KAFKA_CLUSTERS_0_READONLY=false
      - ALLOW_ANONYMOUS_ACCESS=true
    depends_on:
      - kafka

  namenode:
    image: apache/hadoop:3.4.1
    container_name: namenode
    hostname: namenode
    command: |
      sh -c "
        hdfs namenode &
        sleep 15  # Wait for namenode to be ready
        hdfs dfs -mkdir -p /data/raw/traffic
        hdfs dfs -chmod -R 777 /data
        wait
      "
    ports:
      - 9870:9870
      - 8020:8020
    env_file:
      - ./hdfs/config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"

  datanode:
    image: apache/hadoop:3.4.1
    container_name: datanode
    hostname: datanode
    command: ["hdfs", "datanode"]
    env_file:
      - ./hdfs/config

  traffic-generator:  
    build: ./generator
    container_name: traffic-generator
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: kafka:9092

  kafka-to-hdfs:
    build: ./kafka
    container_name: kafka-to-hdfs
    depends_on:
      - kafka
      - namenode
      - datanode

  spark-processor:
    build: ./spark
    container_name: spark-processor
    depends_on:
      - namenode
      - datanode
      # Optional: wait for raw data to exist (or run manually)
    environment:
      - SPARK_MODE=client

  airflow-db:
    image: postgres:15
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-db-data:/var/lib/postgresql/data

  airflow-init:
    image: apache/airflow:2.8.1
    container_name: airflow-init
    depends_on:
      - airflow-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "9098-9879-8211-4532"
    command: |
      bash -c "
      airflow db migrate
      airflow users create --username admin --password admin --firstname admin --lastname admin --role Admin --email admin@example.com
      "

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "9098-9879-8211-4532"
    command: airflow webserver

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: "9098-9879-8211-4532"
    command: airflow scheduler

  traffic-postgres:
    image: postgres:15
    container_name: traffic-postgres
    environment:
      POSTGRES_DB: traffic_analytics
      POSTGRES_USER: traffic
      POSTGRES_PASSWORD: traffic
    ports:
      - "5433:5432"
    volumes:
      - traffic-postgres-data:/var/lib/postgresql/data

  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  global_data:
  airflow-db-data:
  traffic-postgres-data: