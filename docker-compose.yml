services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper-cnt
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"  # <--- Expose this so you can check ZK connectivity
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 10s
      retries: 5
      start_period: 10s

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    ports:
      - "9092:9092"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8089:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      # Optional: enable message viewing (disabled by default for safety)
      - KAFKA_CLUSTERS_0_READONLY=false
      - ALLOW_ANONYMOUS_ACCESS=true
    depends_on:
      - kafka

  traffic-generator:
    build: ./generator
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: kafka:9092

  namenode:
    image: apache/hadoop:3.4.1
    hostname: namenode
    command: |
      sh -c "
        hdfs namenode &
        sleep 15  # Wait for namenode to be ready
        hdfs dfs -mkdir -p /data/raw/traffic
        hdfs dfs -chmod -R 777 /data
        wait
      "
    ports:
      - 9870:9870
      - 8020:8020
    env_file:
      - ./hdfs/config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"

  datanode:
    image: apache/hadoop:3.4.1
    hostname: datanode
    command: ["hdfs", "datanode"]
    env_file:
      - ./hdfs/config

  kafka-to-hdfs:
    build: ./kafka
    depends_on:
      - kafka
      - namenode
      - datanode

  spark-processor:
    build: ./spark
    depends_on:
      - namenode
      - datanode
      # Optional: wait for raw data to exist (or run manually)
    environment:
      - SPARK_MODE=client

volumes:
  global_data: