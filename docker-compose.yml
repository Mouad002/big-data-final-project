services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper-cnt
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"  # <--- Expose this so you can check ZK connectivity
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 10s
      retries: 5
      start_period: 10s

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    ports:
      - "9092:9092"
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8089:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      # Optional: enable message viewing (disabled by default for safety)
      - KAFKA_CLUSTERS_0_READONLY=false
      - ALLOW_ANONYMOUS_ACCESS=true
    depends_on:
      - kafka

  namenode:
    image: apache/hadoop:3.4.1
    hostname: namenode
    command: |
      sh -c "
        hdfs namenode &
        sleep 15  # Wait for namenode to be ready
        hdfs dfs -mkdir -p /data/raw/traffic
        hdfs dfs -chmod -R 777 /data
        wait
      "
    ports:
      - 9870:9870
      - 8020:8020
    env_file:
      - ./hdfs/config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"

  datanode:
    image: apache/hadoop:3.4.1
    hostname: datanode
    command: ["hdfs", "datanode"]
    env_file:
      - ./hdfs/config

  traffic-generator:
    build: ./generator
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: kafka:9092

  kafka-to-hdfs:
    build: ./kafka
    depends_on:
      - kafka
      - namenode
      - datanode

  spark-processor:
    build: ./spark
    depends_on:
      - namenode
      - datanode
      # Optional: wait for raw data to exist (or run manually)
    environment:
      - SPARK_MODE=client

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana-storage:/var/lib/grafana
    depends_on:
      - namenode
  # postgres-airflow:
  #   image: postgres:13
  #   environment:
  #     POSTGRES_USER: airflow
  #     POSTGRES_PASSWORD: airflow
  #     POSTGRES_DB: airflow

  # airflow-webserver:
  #   image: apache/airflow:2.8.1
  #   depends_on:
  #     - postgres-airflow
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: LocalExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
  #     AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./data:/opt/airflow/data
  #   ports:
  #     - "8080:8080"
  #   command: webserver

  # airflow-scheduler:
  #   image: apache/airflow:2.8.1
  #   depends_on:
  #     - airflow-webserver
  #   environment:
  #     AIRFLOW__CORE__EXECUTOR: LocalExecutor
  #     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./data:/opt/airflow/data
  #   command: scheduler
  postgres:
    image: postgres:15
    container_name: postgres-cnt
    environment:
      POSTGRES_DB: smartcity
      POSTGRES_USER: grafana
      POSTGRES_PASSWORD: grafana
    ports:
      - "5432:5432"
    volumes:
      - grafana-postgres-data:/var/lib/postgresql/data
volumes:
  global_data:
  grafana-storage:
  grafana-postgres-data: 